---
title: "Basic Probabilities, Sampling Distribution"
pdf-engine: pdflatex
author: "Mu He"
institute: Xi'an Jiaotong-Liverpool University
header-includes: 
  - \titlegraphic{\vspace{4cm}\flushright\includegraphics[width = 3cm]{xjtluicon.jpg}}
format: 
  beamer:
    aspectratio: 43
    theme: metropolis
    navigation: vertical 
    colortheme: moloch
    background-image: xpbackground.png

bibliography: references.bib
csl: the-annals-of-statistics.csl
---

# Basic Probabilities

## Review

The notes are based on [@pishro-nik2014] [@larsen2005] [@casella2024] [@tao] [@derivati]

### Expectation

> Expectation for Discrete Random Variable: $$
> E(X)=\sum x_i f(x_i)
> $$

> Expectation for Continuous Random Variable: $$
> E(X)=\int x f(x) dx
> $$

# Convergence of Sequence

## Types of Convergence

In this section, we will develop the theoretical background to study the convergence of a sequence of random variables in more detail. In particular, we will define different types of convergence. When we say that the sequence $X_n$ converges to $X$, it means that $X_n$ 's are getting ''closer and closer'' to $X$. Different types of convergence refer to different ways of defining what ''closer'' means. We also discuss how different types of convergence are related.

## Types of Convergence

### Convergence of Sequence

> Convergence of Sequence: A sequence $a_1,a_2,a_3, \cdots, a_n$ converges to a limit $L$ if $$
> \lim_{n\rightarrow \infty} a_n=L
> $$ That is, for any $\epsilon>0$, there exists an $N\in \mathbb{N}$ such that $$
> |a_n-L|<\epsilon, \quad \text{ for all } n> N
> $$

## Basic Probability Theory

### $\sigma$-algrbra, algebra and semi-ring

> Definition: algebra, $\sigma$-algrbra, semi-ring

FAQ: [Exampleof an algebra not a $\sigma$-algebra](https://math.stackexchange.com/questions/233702/example-of-an-algebra-which-is-not-a-%CF%83-algebra)

Not required, just as an introduction. If you are very intersted in probability theory, it is a recommendation to find out why we need these definitions.

## Convergence of Sequence

### Sequence of Random Variables

In statistics, we draw a sample to make inference of the population, then, if we repeatly draw samples, we will have a sequence of samples from the same population, we usually refer them as i.i.d. (independent and identical distributed) or random samples. This can be denoted as

$$
\{\Omega,\Sigma,P\}
$$ where $\Omega$ is the sample space,

$$
\Omega=\{\omega_1,\omega_2,\cdots, \omega_n\}, \quad w_i \text{ are simple(single) events}
$$

$\Sigma$ is the $\sigma$-algebra (You may consider it is set of the sets of simple events in brief) and $P$ is a probability measure.

## Convergence of Sequence

However, if we consider the samples not necessarily from the same population, we may have a sequence of random variables $X_1,X_2,\cdots$, and an correspnded underlying sample space $\Omega$. In particular, each $X_n$ is a function from its $\Omega$, to real numbers through the probability measure $P$.

In other words, a sequence of random variables is in fact a sequence of functions (Mapping, or $P$, or a probability measure) $X_n:\Omega\rightarrow \mathbb{R}$ , such as

$$
P(\omega_i)=x_i, \quad \omega_i \in \Omega \text{ and } \sum x_i=1, \quad i = 1,\cdots,n
$$

## Example: Convergence of Sequence of R.V.

Consider the following random experiment: A fair coin is tossed once. Here, the sample space has only two elements $S=\{H,T\}$. We define a sequence of random variables $X_1,X_2,\cdots$ on this sample space as follows:

$$
 X_n(s) = \left\{
\begin{array}{l l}
\frac{1}{n+1} & \qquad \text{ if }s=H \\
& \qquad \\
1 & \qquad \text{ if }s=T
\end{array} \right.
$$

-   Are the $X_i$ independent?

    No, they are dependent as they are measuring the same coin.

## Example: Convergence of Sequence of R.V.

-   Find the PMF and CDF of $X_n$, $F_{X_n}(x)$ for $n=1,2,3,\cdots$.

The PMF are

$$
P_{{\large X_n}}(x)=P(X_n=x) = \left\{
   \begin{array}{l l}
   \frac{1}{2} & \qquad \textrm{ if }x=\frac{1}{n+1} \\
   & \qquad \\
   \frac{1}{2} & \qquad \textrm{ if }x=1
   \end{array} \right.
$$ Correspondingly, the CDF are

$$
F_{{\large X_n}}(x)=P(X_n \leq x) = \left\{
\begin{array}{l l}
   1 & \qquad \textrm{ if }x \geq 1\\
   & \qquad \\
   \frac{1}{2} & \qquad \textrm{ if }\frac{1}{n+1} \leq x <1 \\
   & \qquad \\
   0 & \qquad \textrm{ if }x< \frac{1}{n+1}
   \end{array} \right.
$$

## Example: Convergence of Sequence of R.V.

-   As $n$ goes to infinity, what does $F_{X_n}(x)$ look like?

## Example: Suppose that $T$ is R.V. as above, derive its p.d.f.

1. If $T$ is given by $\frac{U}{\sqrt{V/k}}$, find the joint density of $U$ and $V$.

2. Find the density function of $T$.

$$
f_{U,V}(u,v) = \underbrace{\frac{1}{(2\pi)^{1/2}} e^{-u^2/2}}_{\text{pdf } N(0,1)}\quad \underbrace{\frac{1}{\Gamma(\frac{k}{2})\,2^{k/2}}\,v^{(k/2)-1}\, e^{-v/2}}_{\text{pdf }\chi^2_k}
$$

Denote 

$$
t=\frac{u}{\sqrt{v/k}}, \quad w=v
$$
   
where

$$
u=t(\frac{w}{k})^{1/2}, \quad v= w
$$
   

The Jacobian matrix can be find as
   
$$
J=\begin{vmatrix}
   \frac{du}{dt} & \frac{du}{dw}\\
   \frac{dv}{dt} & \frac{dv}{dw}\\
   \end{vmatrix}=\begin{vmatrix}
   (\frac{w}{k})^{1/2} & \frac{1}{2}t(\frac{1}{wk})^{1/2}\\
   0&1
   \end{vmatrix}=(\frac{w}{k})^{1/2}
$$
   


## Example: Calculation

The tensile strength for a type of wire is normally distributed with unknown mean $\mu$ and unknown variance $\sigma^2$. Six pieces of wire were randomly selected from a large roll; $Y_i$, the tensile strength for portion $i$, is measured for $i = 1, 2, . . . , 6$. The population mean $\mu$ and variance $\sigma^2$ can be estimated by $\bar{Y}$ and $s^2$, respectively.

Find the approximate probability that $\bar{Y}$ will be within $2S/\sqrt{n}$ of the true population mean $\mu$.

```{r}
pt(2,5)-pt(-2,5)
```

## Example: Calculation

As 

$$
T=\frac{\bar{Y}-\mu}{S/\sqrt{n}}
$$ 

Then 

$$
P(|\bar{Y}-\mu|\leq 2S/\sqrt{n})=P(-2\leq T \leq 2)= P(T\leq 2)-P(T\leq -2)=?
$$

```{r, echo=TRUE}
pt(2,5)-pt(-2,5)
```


## The F Distribution

Suppose that we want to compare the variances of two normal populations based on information contained in independent random samples from the two populations.

> The F Distribution: Let $W_1$ and $W_2$ be independent $\chi^2$ distributed random variables with $v_1$ and $v_2$ degree of freedom. Then, $$
> F=\frac{W_1/v_1}{W_2/v_2}=\frac{(n-1)S^2_1/\sigma^2_1/(n_1-1)}{(n-1)S^2_2/\sigma^2_2/(n_2-1)}=\frac{S^2_1/\sigma^2_1}{S^2_2/\sigma^2_2}
> $$ is an F distribution, $F(v_1=n_1-1,v_2=n_2-1)$.

## Example: Calculation

If there are two popluation with equal variance, we draw two sample with size $n_1=6$ and $n_2=10$, such that

$$
P(\frac{S^2_1}{S^2_2} \leq b)=0.95
$$

What is the value of b?

```{r}
p=0.95
v1=5
v2=9
qf(p,v1,v2)
#pf(q,v1,v2)
```



## Reference
